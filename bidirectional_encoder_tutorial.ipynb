{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjnTwG58+UdsbEtQXYHCJ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TECHNIQUES-IN-TRANSFORMER-MODELS/blob/main/bidirectional_encoder_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "what is auto regressive fashion\n",
        "Auto-regressive fashion refers to the approach of generating a sequence one token at a time, where each token is conditioned on the previously generated tokens. In other words, the model generates the next token in the sequence based on the tokens it has already generated. This is in contrast to non-autoregressive models, which can generate tokens in parallel, without being conditioned on the previous tokens.\n",
        "Auto-regressive models are often used in language modeling and text generation tasks, where the goal is to generate a sequence of words that follows a certain pattern or structure. In these tasks, the model predicts the probability distribution over the vocabulary at each time step, and selects the most likely token based on the previously generated tokens.\n",
        "The advantage of using an auto-regressive approach is that it allows the model to capture complex dependencies between the input and output sequence. However, it also requires the model to make assumptions about the distribution of the data, which can be a limitation in some cases. Additionally, auto-regressive models are often slower to generate output compared to non-autoregressive models, because they need to generate one token at a time.\n"
      ],
      "metadata": {
        "id": "PAA810VJySdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvzmIGsiyP1F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is bidirectional encoder\n",
        "A bidirectional encoder is a type of neural network architecture that allows the model to incorporate information from both the left and right context of the input sequence. In a traditional uni-directional encoder, the input sequence is processed sequentially in one direction (usually left-to-right or right-to-left), and the hidden state at each time step is only influenced by the previous time steps. However, in a bidirectional encoder, the input sequence is processed in both directions simultaneously, and the hidden state at each time step is influenced by both the previous and future time steps.\n",
        "The bidirectional encoder works by passing the input sequence through two separate layers, one that processes the sequence from left-to-right and another that processes the sequence from right-to-left. The outputs of these two layers are then combined to form a single representation of the input sequence that incorporates information from both directions. This combined representation is then used as the input to the subsequent layers of the neural network.\n",
        "Bidirectional encoders are commonly used in natural language processing tasks such as machine translation, text classification, and named entity recognition, where it is important to consider the context of the entire input sequence. The use of a bidirectional encoder allows the model to capture both forward and backward dependencies in the input sequence, leading to improved performance on these tasks."
      ],
      "metadata": {
        "id": "8t-XJjhoyZsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BiDirectionalEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, **kwargs):\n",
        "        super(BiDirectionalEncoder, self).__init__(**kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.forward_lstm = tf.keras.layers.LSTM(units=hidden_size, return_sequences=True)\n",
        "        self.backward_lstm = tf.keras.layers.LSTM(units=hidden_size, return_sequences=True, go_backwards=True)\n",
        "        self.concat = tf.keras.layers.Concatenate(axis=2)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        forward_output = self.forward_lstm(inputs)\n",
        "        backward_output = self.backward_lstm(inputs)\n",
        "        output = self.concat([forward_output, backward_output])\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "pTgGIRTNyaOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In this implementation, we define a custom BiDirectionalEncoder layer that takes an input sequence and processes it using a forward LSTM and a backward LSTM. We then concatenate the outputs of the two LSTMs to obtain the final output sequence.**"
      ],
      "metadata": {
        "id": "Hoxqb8dCzivc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this layer in a model, you can simply add it as a layer in your model, like so:"
      ],
      "metadata": {
        "id": "xwrgM1oSzoNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
        "encoder = BiDirectionalEncoder(hidden_size=hidden_size)(embedding)\n"
      ],
      "metadata": {
        "id": "QdTao91OzcdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}