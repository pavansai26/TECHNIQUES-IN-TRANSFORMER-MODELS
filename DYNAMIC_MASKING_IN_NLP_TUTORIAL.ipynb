{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9uLTdTTWZ0xlYHWw4F1Cv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TECHNIQUES-IN-TRANSFORMER-MODELS/blob/main/DYNAMIC_MASKING_IN_NLP_TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic masking is a technique used in machine learning and natural language processing (NLP) to improve the performance of models by masking certain tokens during training.**"
      ],
      "metadata": {
        "id": "R8WnsCy5Pu33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In NLP, tokens refer to the individual words or subwords that make up a sentence or document.**"
      ],
      "metadata": {
        "id": "PPr_Ks6VP4sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **During training, dynamic masking randomly masks a certain percentage of the input tokens in each batch of training data.**"
      ],
      "metadata": {
        "id": "FBpaYdZ9QBcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This means that the model is not given access to some of the tokens in the input sequence, forcing it to rely on the other tokens to make predictions. The masked tokens are then predicted by the model during training using a process called masked language modeling (MLM).**"
      ],
      "metadata": {
        "id": "Kz9934_lQJt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic masking is required because it helps to prevent the model from overfitting to the training data. Overfitting occurs when a model becomes too specialized to the training data and performs poorly on new, unseen data.**"
      ],
      "metadata": {
        "id": "lzVRYF7iQRhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **By masking some of the tokens during training, the model is forced to generalize its predictions to unseen data, making it more robust and improving its performance on a variety of tasks.**"
      ],
      "metadata": {
        "id": "URf39mPNQWCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Limitations: While dynamic masking is a powerful technique, it is not a panacea for all NLP challenges. It is important to carefully consider the specifics of your training data and task to determine whether dynamic masking is appropriate for your model. Additionally, dynamic masking can be computationally expensive, so it may not be feasible for all training setups.**"
      ],
      "metadata": {
        "id": "F53W-bptQk4E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5z60KFZPk3S"
      },
      "outputs": [],
      "source": []
    }
  ]
}